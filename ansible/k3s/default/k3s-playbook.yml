---
- hosts: "localhost"
  connection: local
  gather_facts: false
  vars_files:
    - vars.yaml
  vars:
    # tf integration (optional).
    terraform_workspace: "{{ lookup('env', 'TF_WORKSPACE') | default('') }}"
    terraform_state_path: "{{ lookup('env', 'TERRAFORM_NODE_SOURCE') | default('tofu/aws/modules/cluster_nodes') }}"
    terraform_state_file: "{{ terraform_state_path }}/terraform.tfstate.d/{{ terraform_workspace }}/terraform.tfstate"
    use_terraform: "{{ terraform_workspace != '' and terraform_workspace is defined }}"

    kube_api_host: "{{ lookup('cloud.terraform.tf_output', 'kube_api_host', state_file=terraform_state_file) if use_terraform else kube_api_host }}"
    ansible_ssh_common_args: "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
  tasks:

    - name: Wait for SSH on node
      wait_for:
        host: "{{ hostvars[item]['ansible_host'] }}"
        port: 22
        timeout: 300
        state: started
      loop: "{{ groups['all'] }}"

- hosts: "all"
  become: true
  gather_facts: false
  tasks:
    - name: Gather minimal OS facts
      setup:
        gather_subset:
          - '!all'
          - '!min'
          - distribution
          - os_family
      when: ansible_os_family is not defined

 
    - name: Disable NetworkManager cloud setup services
      systemd:
        name: "{{ item }}"
        state: stopped
        enabled: no
      loop:
        - nm-cloud-setup.service
        - nm-cloud-setup.timer
      failed_when: false
      when: 
        - ansible_os_family == "RedHat"
        - ansible_distribution_major_version|int >= 8

    - name: Create NetworkManager config for CNI interfaces
      copy:
        content: |
          [keyfile]
          unmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:flannel*
        dest: /etc/NetworkManager/conf.d/99-cni.conf
        mode: '0644'
      when: ansible_os_family == "RedHat"

    - name: Restart NetworkManager
      systemd:
        name: NetworkManager
        state: restarted
      when: ansible_os_family == "RedHat"

- hosts: "master"
  become: true
  gather_facts: false
  vars_files:
    - vars.yaml
  vars:
    # tf integration (optional).
    terraform_workspace: "{{ lookup('env', 'TF_WORKSPACE') | default('') }}"
    terraform_state_path: "{{ lookup('env', 'TERRAFORM_NODE_SOURCE') | default('tofu/aws/modules/cluster_nodes') }}"
    terraform_state_file: "{{ terraform_state_path }}/terraform.tfstate.d/{{ terraform_workspace }}/terraform.tfstate"
    use_terraform: "{{ terraform_workspace != '' and terraform_workspace is defined }}"

    kube_api_host: "{{ lookup('cloud.terraform.tf_output', 'kube_api_host', state_file=terraform_state_file) if use_terraform else kube_api_host }}"
    fqdn: "{{ lookup('cloud.terraform.tf_output', 'fqdn', state_file=terraform_state_file) if use_terraform else (fqdn | default('')) }}"
    node_token_file: "/tmp/node_token.txt" # Local file to store the token
    ansible_ssh_common_args: "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
  tasks:
    - name: Create K3s server directories on master
      file:
        path: "{{ item }}"
        state: directory
        mode: '0755'
      loop:
        - /var/lib/rancher/k3s/server
        - /var/lib/rancher/k3s/server/manifests
        - /var/lib/rancher/k3s/server/logs

      when: '"cp" in ansible_role or "etcd" in ansible_role'
    - name: Read hardening files content
      slurp:
        src: "./{{ item }}"
      register: hardening_files
      loop:
        - policy.yaml
        - audit.yaml
        - cluster-level-pss.yaml
        - ingresspolicy.yaml
        - cis_master_config.yaml
      delegate_to: localhost
      run_once: true

    - name: Transfer and execute init-cluster.sh (initial etcd node)
      script: ./init-cluster.sh
      environment:
        KUBERNETES_VERSION: "{{ kubernetes_version }}"
        KUBE_API_HOST: "{{ kube_api_host }}"
        FQDN: "{{ fqdn }}"
        NODE_ROLE: "{{ ansible_role }}"
        PUBLIC_IP: "{{ ansible_host }}" 
        SERVER_FLAGS: "{{ server_flags | default('') }}"
        CHANNEL: "{{ channel | default('') }}"
        # Pass hardening file contents as environment variables
        POLICY_YAML_CONTENT: "{{ (hardening_files.results | selectattr('item', 'equalto', 'policy.yaml') | first).content | b64decode }}"
        AUDIT_YAML_CONTENT: "{{ (hardening_files.results | selectattr('item', 'equalto', 'audit.yaml') | first).content | b64decode }}"
        CLUSTER_LEVEL_PSS_YAML_CONTENT: "{{ (hardening_files.results | selectattr('item', 'equalto', 'cluster-level-pss.yaml') | first).content | b64decode }}"
        INGRESSPOLICY_YAML_CONTENT: "{{ (hardening_files.results | selectattr('item', 'equalto', 'ingresspolicy.yaml') | first).content | b64decode }}"
        CIS_MASTER_CONFIG_YAML_CONTENT: "{{ (hardening_files.results | selectattr('item', 'equalto', 'cis_master_config.yaml') | first).content | b64decode }}"
      register: script_result

    - name: Display script output (stdout)
      debug:
        var: script_result.stdout_lines

    - name: Check script return code
      debug:
        msg: "Script returned: {{ script_result.rc }}"

    - name: Display script errors (stderr) if any
      debug:
        var: script_result.stderr_lines
      when: script_result.rc != 0

    - name: Get node-token from master
      raw: cat /var/lib/rancher/k3s/server/node-token
      register: node_token_output

    - name: Save node_token to local file
      delegate_to: localhost
      run_once: true
      become: no
      shell: echo "{{ node_token_output.stdout }}" > "{{ node_token_file }}"

    - name: Read file contents into a variable
      raw: cat /etc/rancher/k3s/k3s.yaml
      register: file_contents

    - name: Save file contents locally
      delegate_to: localhost
      run_once: true
      become: no
      shell: echo "{{ file_contents.stdout }}" > "{{ kubeconfig_file }}"

- hosts: "all"
  become: true
  gather_facts: false
  vars_files:
    - vars.yaml
  vars:
    # tf integration (optional).
    terraform_workspace: "{{ lookup('env', 'TF_WORKSPACE') | default('') }}"
    terraform_state_path: "{{ lookup('env', 'TERRAFORM_NODE_SOURCE') | default('tofu/aws/modules/cluster_nodes') }}"
    terraform_state_file: "{{ terraform_state_path }}/terraform.tfstate.d/{{ terraform_workspace }}/terraform.tfstate"
    use_terraform: "{{ terraform_workspace != '' and terraform_workspace is defined }}"

    kube_api_host: "{{ lookup('cloud.terraform.tf_output', 'kube_api_host', state_file=terraform_state_file) if use_terraform else kube_api_host }}"
    fqdn: "{{ lookup('cloud.terraform.tf_output', 'fqdn', state_file=terraform_state_file) if use_terraform else (fqdn | default('')) }}"
    server_host: "{{ server_host_override | default((groups['all'] | selectattr('terraform_facts.output_nodes.value[inventory_hostname].is_server', 'defined') | selectattr('terraform_facts.output_nodes.value[inventory_hostname].is_server', 'true') | map('inventory_hostname') | list)[0] if use_terraform else groups['master'][0]) }}"
    node_token_file: "/tmp/node_token.txt" # Local file to store the token
    ansible_ssh_common_args: "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
  tasks:
    - name: Read node_token from local file
      delegate_to: localhost
      run_once: true
      become: false
      slurp:
        src: "{{ node_token_file }}"
      register: node_token_file_content

    - name: Set node_token fact from file content
      set_fact:
        node_token: "{{ node_token_file_content.content | b64decode | trim }}"
      run_once: true
      delegate_to: localhost

    - name: Create K3s server directories on server nodes
      file:
        path: "{{ item }}"
        state: directory
        mode: '0755'
      loop:
        - /var/lib/rancher/k3s/server
        - /var/lib/rancher/k3s/server/manifests
        - /var/lib/rancher/k3s/server/logs
      when: '"cp" in ansible_role or "etcd" in ansible_role'

    - name: Read hardening files content for server nodes
      slurp:
        src: "./{{ item }}"
      register: hardening_files_servers
      loop:
        - policy.yaml
        - audit.yaml
        - cluster-level-pss.yaml
        - ingresspolicy.yaml
        - cis_master_config.yaml
      delegate_to: localhost
      become: no
      run_once: true
      when: '"cp" in ansible_role or "etcd" in ansible_role'

    - name: Transfer and execute init-server.sh (Server)
      script: ./init-server.sh
      environment:
        KUBERNETES_VERSION: "{{ kubernetes_version }}"
        KUBE_API_HOST: "{{ kube_api_host }}"
        FQDN: "{{ fqdn }}"
        NODE_TOKEN: "{{ node_token }}"
        NODE_ROLE: "{{ ansible_role }}"
        PUBLIC_IP: "{{ ansible_host }}"
        SERVER_FLAGS: "{{ server_flags | default('') }}"
        CHANNEL: "{{ channel | default('v1') }}"
        # Pass hardening file contents as environment variables
        POLICY_YAML_CONTENT: "{{ (hardening_files_servers.results | selectattr('item', 'equalto', 'policy.yaml') | first).content | b64decode }}"
        AUDIT_YAML_CONTENT: "{{ (hardening_files_servers.results | selectattr('item', 'equalto', 'audit.yaml') | first).content | b64decode }}"
        CLUSTER_LEVEL_PSS_YAML_CONTENT: "{{ (hardening_files_servers.results | selectattr('item', 'equalto', 'cluster-level-pss.yaml') | first).content | b64decode }}"
        INGRESSPOLICY_YAML_CONTENT: "{{ (hardening_files_servers.results | selectattr('item', 'equalto', 'ingresspolicy.yaml') | first).content | b64decode }}"
        CIS_MASTER_CONFIG_YAML_CONTENT: "{{ (hardening_files_servers.results | selectattr('item', 'equalto', 'cis_master_config.yaml') | first).content | b64decode }}"
      register: script_result
      when: 
        - inventory_hostname != "master"
        - '"cp" in ansible_role or "etcd" in ansible_role'
        
    - name: Transfer and execute init-agent.sh (Agent)
      script: ./init-agent.sh
      environment:
        KUBERNETES_VERSION: "{{ kubernetes_version }}"
        KUBE_API_HOST: "{{ kube_api_host }}"
        FQDN: "{{ fqdn }}"
        NODE_TOKEN: "{{ node_token }}"
        PUBLIC_IP: "{{ ansible_host }}"
        WORKER_FLAGS: "{{ worker_flags | default('') }}"
        CHANNEL: "{{ channel | default('v1') }}"
      register: script_result
      when: ansible_role == "worker"

- hosts: localhost
  connection: local
  gather_facts: false
  vars_files:
    - vars.yaml
  tasks:
    - name: Wait until all K3s system pods are Running or Succeeded
      kubernetes.core.k8s_info:
        kubeconfig: "{{ kubeconfig_file }}"
        api_version: v1
        kind: Pod
        namespace: kube-system
      register: k3s_pods_check
      until: >
        k3s_pods_check.resources is defined and
        (k3s_pods_check.resources | rejectattr('status.phase', 'in', ['Running', 'Succeeded']) | list | length == 0)
      retries: 5
      delay: 60
